{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "molecular-moscow",
   "metadata": {},
   "source": [
    "# Imputation Plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-basis",
   "metadata": {},
   "source": [
    "Missing data is a crucial issue when applying machine learning algorithms to real-world datasets.\n",
    "\n",
    "**HyperImpute** provides a set of default imputation plugins and can be extended with any number of other plugins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-hygiene",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wanted-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from hyperimpute.plugins.utils.metrics import RMSE\n",
    "from hyperimpute.plugins.utils.simulate import simulate_nan\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-console",
   "metadata": {},
   "source": [
    "### Loading the Imputation plugins\n",
    "\n",
    "Make sure that you have installed HyperImpute in your workspace.\n",
    "\n",
    "You can do that by running `pip install .` in the root of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coated-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperimpute.plugins.imputers import Imputers, ImputerPlugin\n",
    "\n",
    "imputers = Imputers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-booth",
   "metadata": {},
   "source": [
    "### List the existing plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incident-familiar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['softimpute',\n",
       " 'hyperimpute',\n",
       " 'mice',\n",
       " 'nop',\n",
       " 'most_frequent',\n",
       " 'missforest',\n",
       " 'sinkhorn',\n",
       " 'EM',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'ice',\n",
       " 'gain']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputers.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-karma",
   "metadata": {},
   "source": [
    "### Adding a new Imputation plugin\n",
    "\n",
    "By default, HyperImpute automatically loads the imputation plugins with the pattern `hyperimpute/plugins/imputers/plugin_*`. \n",
    "\n",
    "Alternatively, you can call `Imputers().add(<name>, <ImputerPlugin derived class>)` at runtime.\n",
    "\n",
    "Next, we show two examples of custom Imputation plugins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sophisticated-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ice_plugin = \"custom_ice\"\n",
    "\n",
    "\n",
    "class NewPlugin(ImputerPlugin):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        lr = LinearRegression()\n",
    "        self._model = IterativeImputer(\n",
    "            estimator=lr, max_iter=500, tol=1e-10, imputation_order=\"roman\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def name():\n",
    "        return custom_ice_plugin\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperparameter_space():\n",
    "        return []\n",
    "\n",
    "    def _fit(self, *args, **kwargs) -> \"NewPlugin\":\n",
    "        self._model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def _transform(self, *args, **kwargs):\n",
    "        return self._model.transform(*args, **kwargs)\n",
    "\n",
    "    def save(self) -> bytes:\n",
    "        raise NotImplemented(\"placeholder\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, buff: bytes) -> \"NewPlugin\":\n",
    "        raise NotImplemented(\"placeholder\")\n",
    "\n",
    "\n",
    "imputers.add(custom_ice_plugin, NewPlugin)\n",
    "\n",
    "assert imputers.get(custom_ice_plugin) is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-broad",
   "metadata": {},
   "source": [
    "### List the existing plugins\n",
    "\n",
    "Now we should see the new plugins loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "loved-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['softimpute',\n",
       " 'hyperimpute',\n",
       " 'mice',\n",
       " 'nop',\n",
       " 'most_frequent',\n",
       " 'missforest',\n",
       " 'sinkhorn',\n",
       " 'EM',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'ice',\n",
       " 'gain',\n",
       " 'custom_ice']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputers.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-tulsa",
   "metadata": {},
   "source": [
    "### Testing the performance\n",
    "\n",
    "We simulate some testing datasets using 3 amputation strategies:\n",
    "- **Missing Completely At Random** (MCAR) if the probability of being missing is the same for all observations\n",
    "- **Missing At Random** (MAR) if the probability of being missing only depends on observed values.\n",
    "- **Missing Not At Random** (MNAR) if the unavailability of the data depends on both observed and unobserved data such as its value itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-expert",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "preceding-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "preproc = MinMaxScaler()\n",
    "\n",
    "\n",
    "def dataset():\n",
    "    X, y = load_breast_cancer(return_X_y=True)\n",
    "    X = np.asarray(preproc.fit_transform(X, y))\n",
    "    return train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "def ampute(x, mechanism, p_miss):\n",
    "    x_simulated = simulate_nan(x, p_miss, mechanism)\n",
    "\n",
    "    mask = x_simulated[\"mask\"]\n",
    "    x_miss = x_simulated[\"X_incomp\"]\n",
    "\n",
    "    return x, x_miss, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chief-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "headers = [\"Plugin\"]\n",
    "\n",
    "pct = 0.3\n",
    "\n",
    "mechanisms = [\"MAR\", \"MNAR\", \"MCAR\"]\n",
    "percentages = [pct]\n",
    "\n",
    "plugins = imputers.list()  # default plugins\n",
    "\n",
    "X_train, X_test, y_train, y_test = dataset()\n",
    "\n",
    "for ampute_mechanism in mechanisms:\n",
    "    for p_miss in percentages:\n",
    "        if ampute_mechanism not in datasets:\n",
    "            datasets[ampute_mechanism] = {}\n",
    "\n",
    "        headers.append(ampute_mechanism + \"-\" + str(p_miss))\n",
    "        datasets[ampute_mechanism][p_miss] = ampute(X_train, ampute_mechanism, p_miss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-finding",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "We compare the methods in terms of root mean squared error (RMSE) to the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "recovered-tonight",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [23:58<00:00, 110.69s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "duration = []\n",
    "\n",
    "for plugin in tqdm(plugins):\n",
    "    plugin_results = [plugin]\n",
    "    plugin_duration = [plugin]\n",
    "\n",
    "    for ampute_mechanism in mechanisms:\n",
    "        for p_miss in percentages:\n",
    "            ctx = imputers.get(plugin)\n",
    "            x, x_miss, mask = datasets[ampute_mechanism][p_miss]\n",
    "\n",
    "            start = time.time() * 1000\n",
    "            x_imp = ctx.fit_transform(x_miss)\n",
    "\n",
    "            plugin_duration.append(round(time.time() * 1000 - start, 4))\n",
    "            plugin_results.append(RMSE(x_imp.to_numpy(), x, mask))\n",
    "\n",
    "    results.append(plugin_results)\n",
    "    duration.append(plugin_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-amount",
   "metadata": {},
   "source": [
    "### Reconstruction error(RMSE)\n",
    "\n",
    "__Interpretation__ : The following table shows the reconstruction error -  the __Root Mean Square Error(RMSE)__ for each method applied on the original full dataset and the imputed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "normal-decline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Plugin       </th><th style=\"text-align: right;\">    MAR-0.3</th><th style=\"text-align: right;\">   MNAR-0.3</th><th style=\"text-align: right;\">   MCAR-0.3</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>softimpute   </td><td style=\"text-align: right;\">  0.112766 </td><td style=\"text-align: right;\">  0.109049 </td><td style=\"text-align: right;\">  0.0961995</td></tr>\n",
       "<tr><td>hyperimpute  </td><td style=\"text-align: right;\">  0.0818665</td><td style=\"text-align: right;\">  0.0776259</td><td style=\"text-align: right;\">  0.0619054</td></tr>\n",
       "<tr><td>mice         </td><td style=\"text-align: right;\">  0.0703866</td><td style=\"text-align: right;\">  0.0850556</td><td style=\"text-align: right;\">  0.0733156</td></tr>\n",
       "<tr><td>nop          </td><td style=\"text-align: right;\">nan        </td><td style=\"text-align: right;\">nan        </td><td style=\"text-align: right;\">nan        </td></tr>\n",
       "<tr><td>most_frequent</td><td style=\"text-align: right;\">  0.242866 </td><td style=\"text-align: right;\">  0.231878 </td><td style=\"text-align: right;\">  0.209099 </td></tr>\n",
       "<tr><td>missforest   </td><td style=\"text-align: right;\">  0.0859716</td><td style=\"text-align: right;\">  0.0881004</td><td style=\"text-align: right;\">  0.0744402</td></tr>\n",
       "<tr><td>sinkhorn     </td><td style=\"text-align: right;\">  0.093691 </td><td style=\"text-align: right;\">  0.0885909</td><td style=\"text-align: right;\">  0.0762017</td></tr>\n",
       "<tr><td>EM           </td><td style=\"text-align: right;\">  0.0567481</td><td style=\"text-align: right;\">  0.0653695</td><td style=\"text-align: right;\">  0.0524116</td></tr>\n",
       "<tr><td>mean         </td><td style=\"text-align: right;\">  0.184329 </td><td style=\"text-align: right;\">  0.164505 </td><td style=\"text-align: right;\">  0.14423  </td></tr>\n",
       "<tr><td>median       </td><td style=\"text-align: right;\">  0.197736 </td><td style=\"text-align: right;\">  0.173701 </td><td style=\"text-align: right;\">  0.149625 </td></tr>\n",
       "<tr><td>ice          </td><td style=\"text-align: right;\">  0.0575022</td><td style=\"text-align: right;\">  0.071648 </td><td style=\"text-align: right;\">  0.0567319</td></tr>\n",
       "<tr><td>gain         </td><td style=\"text-align: right;\">  0.122367 </td><td style=\"text-align: right;\">  0.193884 </td><td style=\"text-align: right;\">  0.088739 </td></tr>\n",
       "<tr><td>custom_ice   </td><td style=\"text-align: right;\">  0.0597237</td><td style=\"text-align: right;\">  0.07732  </td><td style=\"text-align: right;\">  0.0598584</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(tabulate.tabulate(results, headers=headers, tablefmt=\"html\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-favor",
   "metadata": {},
   "source": [
    "### XGBoost test score after imputation\n",
    "\n",
    "__Interpretation__ The following table shows different metrics on the test set for an XGBoost classifier, after imputing the dataset with each method.\n",
    "Metrics:\n",
    " - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nonprofit-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def get_metrics(X_train, y_train, X_test, y_test):\n",
    "    xgb_clf = xgb.XGBClassifier(verbosity=0)\n",
    "    xgb_clf = xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "    score = xgb_clf.score(X_test, y_test)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "    auroc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    prec, recall, thresholds = metrics.precision_recall_curve(y_test, y_pred)\n",
    "    aurpc = metrics.auc(recall, prec)\n",
    "\n",
    "    return score, auroc, aurpc\n",
    "\n",
    "\n",
    "metrics_headers = [\"Plugin\", \"Accuracy\", \"AUROC\", \"AURPC\"]\n",
    "xgboost_test_score = []\n",
    "\n",
    "\n",
    "x, x_miss, mask = datasets[\"MAR\"][pct]\n",
    "\n",
    "xgboost_test_score.append(\n",
    "    [\"original dataset\", *get_metrics(X_train, y_train, X_test, y_test)]\n",
    ")\n",
    "\n",
    "for plugin in plugins:\n",
    "    X_train_imp = imputers.get(plugin).fit_transform(x_miss.copy())\n",
    "\n",
    "    score, auroc, aurpc = get_metrics(X_train_imp, y_train, X_test, y_test)\n",
    "\n",
    "    xgboost_test_score.append([plugin, score, auroc, aurpc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77086d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Plugin          </th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">   AUROC</th><th style=\"text-align: right;\">   AURPC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>original dataset</td><td style=\"text-align: right;\">  0.991228</td><td style=\"text-align: right;\">0.989796</td><td style=\"text-align: right;\">0.992424</td></tr>\n",
       "<tr><td>softimpute      </td><td style=\"text-align: right;\">  0.991228</td><td style=\"text-align: right;\">0.989796</td><td style=\"text-align: right;\">0.992424</td></tr>\n",
       "<tr><td>hyperimpute     </td><td style=\"text-align: right;\">  0.982456</td><td style=\"text-align: right;\">0.982104</td><td style=\"text-align: right;\">0.989001</td></tr>\n",
       "<tr><td>mice            </td><td style=\"text-align: right;\">  0.991228</td><td style=\"text-align: right;\">0.989796</td><td style=\"text-align: right;\">0.992424</td></tr>\n",
       "<tr><td>nop             </td><td style=\"text-align: right;\">  0.991228</td><td style=\"text-align: right;\">0.989796</td><td style=\"text-align: right;\">0.992424</td></tr>\n",
       "<tr><td>most_frequent   </td><td style=\"text-align: right;\">  0.991228</td><td style=\"text-align: right;\">0.989796</td><td style=\"text-align: right;\">0.992424</td></tr>\n",
       "<tr><td>missforest      </td><td style=\"text-align: right;\">  0.991228</td><td style=\"text-align: right;\">0.989796</td><td style=\"text-align: right;\">0.992424</td></tr>\n",
       "<tr><td>sinkhorn        </td><td style=\"text-align: right;\">  0.991228</td><td style=\"text-align: right;\">0.989796</td><td style=\"text-align: right;\">0.992424</td></tr>\n",
       "<tr><td>EM              </td><td style=\"text-align: right;\">  0.982456</td><td style=\"text-align: right;\">0.982104</td><td style=\"text-align: right;\">0.989001</td></tr>\n",
       "<tr><td>mean            </td><td style=\"text-align: right;\">  0.973684</td><td style=\"text-align: right;\">0.9719  </td><td style=\"text-align: right;\">0.981542</td></tr>\n",
       "<tr><td>median          </td><td style=\"text-align: right;\">  0.973684</td><td style=\"text-align: right;\">0.974411</td><td style=\"text-align: right;\">0.985575</td></tr>\n",
       "<tr><td>ice             </td><td style=\"text-align: right;\">  0.991228</td><td style=\"text-align: right;\">0.992308</td><td style=\"text-align: right;\">0.996694</td></tr>\n",
       "<tr><td>gain            </td><td style=\"text-align: right;\">  0.991228</td><td style=\"text-align: right;\">0.989796</td><td style=\"text-align: right;\">0.992424</td></tr>\n",
       "<tr><td>custom_ice      </td><td style=\"text-align: right;\">  1       </td><td style=\"text-align: right;\">1       </td><td style=\"text-align: right;\">1       </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    HTML(\n",
    "        tabulate.tabulate(xgboost_test_score, headers=metrics_headers, tablefmt=\"html\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-million",
   "metadata": {},
   "source": [
    "### Duration(ms) results\n",
    "\n",
    "__Info__ : Here we measure the duration of imputing the dataset with each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "viral-suggestion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Plugin       </th><th style=\"text-align: right;\">    MAR-0.3</th><th style=\"text-align: right;\">   MNAR-0.3</th><th style=\"text-align: right;\">   MCAR-0.3</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>softimpute   </td><td style=\"text-align: right;\"> 27735.8   </td><td style=\"text-align: right;\"> 16999.7   </td><td style=\"text-align: right;\"> 16842     </td></tr>\n",
       "<tr><td>hyperimpute  </td><td style=\"text-align: right;\">104270     </td><td style=\"text-align: right;\">108181     </td><td style=\"text-align: right;\"> 70727.8   </td></tr>\n",
       "<tr><td>mice         </td><td style=\"text-align: right;\"> 87594.6   </td><td style=\"text-align: right;\">155285     </td><td style=\"text-align: right;\">149379     </td></tr>\n",
       "<tr><td>nop          </td><td style=\"text-align: right;\">     0.3501</td><td style=\"text-align: right;\">     0.1931</td><td style=\"text-align: right;\">     0.1396</td></tr>\n",
       "<tr><td>most_frequent</td><td style=\"text-align: right;\">     7.144 </td><td style=\"text-align: right;\">     6.1777</td><td style=\"text-align: right;\">     6.4756</td></tr>\n",
       "<tr><td>missforest   </td><td style=\"text-align: right;\"> 56601.3   </td><td style=\"text-align: right;\"> 65379.5   </td><td style=\"text-align: right;\"> 65258.5   </td></tr>\n",
       "<tr><td>sinkhorn     </td><td style=\"text-align: right;\"> 23047.8   </td><td style=\"text-align: right;\"> 22940     </td><td style=\"text-align: right;\"> 23160.6   </td></tr>\n",
       "<tr><td>EM           </td><td style=\"text-align: right;\"> 76708.2   </td><td style=\"text-align: right;\"> 75736.8   </td><td style=\"text-align: right;\"> 74979.7   </td></tr>\n",
       "<tr><td>mean         </td><td style=\"text-align: right;\">    12.6116</td><td style=\"text-align: right;\">     5.5164</td><td style=\"text-align: right;\">     6.0649</td></tr>\n",
       "<tr><td>median       </td><td style=\"text-align: right;\">     7.5566</td><td style=\"text-align: right;\">     7.6375</td><td style=\"text-align: right;\">     7.6201</td></tr>\n",
       "<tr><td>ice          </td><td style=\"text-align: right;\"> 38296.4   </td><td style=\"text-align: right;\">  1521.75  </td><td style=\"text-align: right;\">  1071.36  </td></tr>\n",
       "<tr><td>gain         </td><td style=\"text-align: right;\"> 50893.8   </td><td style=\"text-align: right;\"> 48985.7   </td><td style=\"text-align: right;\"> 48258.8   </td></tr>\n",
       "<tr><td>custom_ice   </td><td style=\"text-align: right;\"> 27956.4   </td><td style=\"text-align: right;\">   553.358 </td><td style=\"text-align: right;\">   418.633 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(tabulate.tabulate(duration, headers=headers, tablefmt=\"html\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-description",
   "metadata": {},
   "source": [
    "## Debugging\n",
    "\n",
    "HyperImpute supports **debug** logging. __WARNING__: Don't use it for release builds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "constitutional-toyota",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-12-28T13:37:41.494917+0200][325578][DEBUG] EMPlugin._fit took 0.0 seconds\n",
      "[2021-12-28T13:37:41.516348+0200][325578][DEBUG] EM converged after 1 iterations.\n",
      "[2021-12-28T13:37:41.517241+0200][325578][DEBUG] EMPlugin._transform took 0.0215 seconds\n",
      "[2021-12-28T13:37:58.960419+0200][325578][DEBUG] SoftImputePlugin._fit took 17.4421 seconds\n",
      "[2021-12-28T13:38:05.198815+0200][325578][DEBUG] SoftImputePlugin._transform took 6.235 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.552274</td>\n",
       "      <td>0.471933</td>\n",
       "      <td>0.552208</td>\n",
       "      <td>0.419871</td>\n",
       "      <td>0.496919</td>\n",
       "      <td>0.457702</td>\n",
       "      <td>0.360506</td>\n",
       "      <td>0.501491</td>\n",
       "      <td>0.427778</td>\n",
       "      <td>0.221778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529705</td>\n",
       "      <td>0.247868</td>\n",
       "      <td>0.498979</td>\n",
       "      <td>0.339609</td>\n",
       "      <td>0.638777</td>\n",
       "      <td>0.467357</td>\n",
       "      <td>0.384144</td>\n",
       "      <td>0.817182</td>\n",
       "      <td>0.323082</td>\n",
       "      <td>0.216479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.248471</td>\n",
       "      <td>0.390260</td>\n",
       "      <td>0.317877</td>\n",
       "      <td>0.195080</td>\n",
       "      <td>0.343685</td>\n",
       "      <td>0.153580</td>\n",
       "      <td>0.034255</td>\n",
       "      <td>0.094235</td>\n",
       "      <td>0.230808</td>\n",
       "      <td>0.176706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263252</td>\n",
       "      <td>0.486674</td>\n",
       "      <td>0.238358</td>\n",
       "      <td>0.130333</td>\n",
       "      <td>0.379912</td>\n",
       "      <td>0.120315</td>\n",
       "      <td>0.049768</td>\n",
       "      <td>0.273643</td>\n",
       "      <td>0.130298</td>\n",
       "      <td>0.138594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.416442</td>\n",
       "      <td>0.446398</td>\n",
       "      <td>0.427821</td>\n",
       "      <td>0.347622</td>\n",
       "      <td>0.567572</td>\n",
       "      <td>0.386529</td>\n",
       "      <td>0.499766</td>\n",
       "      <td>0.471123</td>\n",
       "      <td>0.523232</td>\n",
       "      <td>0.491786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436144</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>0.397878</td>\n",
       "      <td>0.267106</td>\n",
       "      <td>0.588644</td>\n",
       "      <td>0.451349</td>\n",
       "      <td>0.587540</td>\n",
       "      <td>0.698969</td>\n",
       "      <td>0.368858</td>\n",
       "      <td>0.267273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.352075</td>\n",
       "      <td>0.374621</td>\n",
       "      <td>0.350287</td>\n",
       "      <td>0.211665</td>\n",
       "      <td>0.405254</td>\n",
       "      <td>0.290534</td>\n",
       "      <td>0.219963</td>\n",
       "      <td>0.290209</td>\n",
       "      <td>0.413636</td>\n",
       "      <td>0.293597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298826</td>\n",
       "      <td>0.502132</td>\n",
       "      <td>0.294288</td>\n",
       "      <td>0.157589</td>\n",
       "      <td>0.475005</td>\n",
       "      <td>0.267107</td>\n",
       "      <td>0.249416</td>\n",
       "      <td>0.537801</td>\n",
       "      <td>0.227282</td>\n",
       "      <td>0.252460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.171281</td>\n",
       "      <td>0.312479</td>\n",
       "      <td>0.176145</td>\n",
       "      <td>0.101191</td>\n",
       "      <td>0.399476</td>\n",
       "      <td>0.292375</td>\n",
       "      <td>0.149649</td>\n",
       "      <td>0.131312</td>\n",
       "      <td>0.435354</td>\n",
       "      <td>0.361038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155112</td>\n",
       "      <td>0.291045</td>\n",
       "      <td>0.138802</td>\n",
       "      <td>0.058887</td>\n",
       "      <td>0.331044</td>\n",
       "      <td>0.217530</td>\n",
       "      <td>0.155045</td>\n",
       "      <td>0.272371</td>\n",
       "      <td>0.271043</td>\n",
       "      <td>0.212379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0.277770</td>\n",
       "      <td>0.394319</td>\n",
       "      <td>0.268399</td>\n",
       "      <td>0.157370</td>\n",
       "      <td>0.394079</td>\n",
       "      <td>0.195632</td>\n",
       "      <td>0.143533</td>\n",
       "      <td>0.092793</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.235468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230167</td>\n",
       "      <td>0.399520</td>\n",
       "      <td>0.205289</td>\n",
       "      <td>0.113203</td>\n",
       "      <td>0.400683</td>\n",
       "      <td>0.161355</td>\n",
       "      <td>0.146805</td>\n",
       "      <td>0.192474</td>\n",
       "      <td>0.181944</td>\n",
       "      <td>0.173619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>0.222869</td>\n",
       "      <td>0.498140</td>\n",
       "      <td>0.225140</td>\n",
       "      <td>0.133331</td>\n",
       "      <td>0.640697</td>\n",
       "      <td>0.416600</td>\n",
       "      <td>0.105787</td>\n",
       "      <td>0.225199</td>\n",
       "      <td>0.540909</td>\n",
       "      <td>0.507372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179651</td>\n",
       "      <td>0.537580</td>\n",
       "      <td>0.177848</td>\n",
       "      <td>0.074346</td>\n",
       "      <td>0.697550</td>\n",
       "      <td>0.288937</td>\n",
       "      <td>0.214834</td>\n",
       "      <td>0.449485</td>\n",
       "      <td>0.244037</td>\n",
       "      <td>0.292929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>0.116191</td>\n",
       "      <td>0.291173</td>\n",
       "      <td>0.110773</td>\n",
       "      <td>0.057306</td>\n",
       "      <td>0.435768</td>\n",
       "      <td>0.177971</td>\n",
       "      <td>0.063496</td>\n",
       "      <td>0.069881</td>\n",
       "      <td>0.225253</td>\n",
       "      <td>0.413437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>0.346482</td>\n",
       "      <td>0.126401</td>\n",
       "      <td>0.062525</td>\n",
       "      <td>0.410289</td>\n",
       "      <td>0.075298</td>\n",
       "      <td>0.091374</td>\n",
       "      <td>0.173608</td>\n",
       "      <td>0.175241</td>\n",
       "      <td>0.172635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>0.232335</td>\n",
       "      <td>0.387555</td>\n",
       "      <td>0.225278</td>\n",
       "      <td>0.123139</td>\n",
       "      <td>0.407150</td>\n",
       "      <td>0.189620</td>\n",
       "      <td>0.059864</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.484343</td>\n",
       "      <td>0.272536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182142</td>\n",
       "      <td>0.404851</td>\n",
       "      <td>0.172718</td>\n",
       "      <td>0.082997</td>\n",
       "      <td>0.471703</td>\n",
       "      <td>0.185707</td>\n",
       "      <td>0.092971</td>\n",
       "      <td>0.283952</td>\n",
       "      <td>0.297654</td>\n",
       "      <td>0.121147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0.985050</td>\n",
       "      <td>0.568939</td>\n",
       "      <td>0.901182</td>\n",
       "      <td>0.724600</td>\n",
       "      <td>0.464058</td>\n",
       "      <td>0.480954</td>\n",
       "      <td>0.553293</td>\n",
       "      <td>0.950795</td>\n",
       "      <td>0.452525</td>\n",
       "      <td>0.029660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932739</td>\n",
       "      <td>0.308102</td>\n",
       "      <td>0.890931</td>\n",
       "      <td>0.749312</td>\n",
       "      <td>0.482300</td>\n",
       "      <td>0.549534</td>\n",
       "      <td>0.581011</td>\n",
       "      <td>0.947079</td>\n",
       "      <td>0.315622</td>\n",
       "      <td>0.163458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.552274  0.471933  0.552208  0.419871  0.496919  0.457702  0.360506   \n",
       "1    0.248471  0.390260  0.317877  0.195080  0.343685  0.153580  0.034255   \n",
       "2    0.416442  0.446398  0.427821  0.347622  0.567572  0.386529  0.499766   \n",
       "3    0.352075  0.374621  0.350287  0.211665  0.405254  0.290534  0.219963   \n",
       "4    0.171281  0.312479  0.176145  0.101191  0.399476  0.292375  0.149649   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "450  0.277770  0.394319  0.268399  0.157370  0.394079  0.195632  0.143533   \n",
       "451  0.222869  0.498140  0.225140  0.133331  0.640697  0.416600  0.105787   \n",
       "452  0.116191  0.291173  0.110773  0.057306  0.435768  0.177971  0.063496   \n",
       "453  0.232335  0.387555  0.225278  0.123139  0.407150  0.189620  0.059864   \n",
       "454  0.985050  0.568939  0.901182  0.724600  0.464058  0.480954  0.553293   \n",
       "\n",
       "           7         8         9   ...        20        21        22  \\\n",
       "0    0.501491  0.427778  0.221778  ...  0.529705  0.247868  0.498979   \n",
       "1    0.094235  0.230808  0.176706  ...  0.263252  0.486674  0.238358   \n",
       "2    0.471123  0.523232  0.491786  ...  0.436144  0.492537  0.397878   \n",
       "3    0.290209  0.413636  0.293597  ...  0.298826  0.502132  0.294288   \n",
       "4    0.131312  0.435354  0.361038  ...  0.155112  0.291045  0.138802   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "450  0.092793  0.262626  0.235468  ...  0.230167  0.399520  0.205289   \n",
       "451  0.225199  0.540909  0.507372  ...  0.179651  0.537580  0.177848   \n",
       "452  0.069881  0.225253  0.413437  ...  0.145500  0.346482  0.126401   \n",
       "453  0.108300  0.484343  0.272536  ...  0.182142  0.404851  0.172718   \n",
       "454  0.950795  0.452525  0.029660  ...  0.932739  0.308102  0.890931   \n",
       "\n",
       "           23        24        25        26        27        28        29  \n",
       "0    0.339609  0.638777  0.467357  0.384144  0.817182  0.323082  0.216479  \n",
       "1    0.130333  0.379912  0.120315  0.049768  0.273643  0.130298  0.138594  \n",
       "2    0.267106  0.588644  0.451349  0.587540  0.698969  0.368858  0.267273  \n",
       "3    0.157589  0.475005  0.267107  0.249416  0.537801  0.227282  0.252460  \n",
       "4    0.058887  0.331044  0.217530  0.155045  0.272371  0.271043  0.212379  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "450  0.113203  0.400683  0.161355  0.146805  0.192474  0.181944  0.173619  \n",
       "451  0.074346  0.697550  0.288937  0.214834  0.449485  0.244037  0.292929  \n",
       "452  0.062525  0.410289  0.075298  0.091374  0.173608  0.175241  0.172635  \n",
       "453  0.082997  0.471703  0.185707  0.092971  0.283952  0.297654  0.121147  \n",
       "454  0.749312  0.482300  0.549534  0.581011  0.947079  0.315622  0.163458  \n",
       "\n",
       "[455 rows x 30 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hyperimpute import logger\n",
    "\n",
    "imputers = Imputers()\n",
    "\n",
    "logger.add(sink=sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "x, x_miss, mask = datasets[\"MAR\"][pct]\n",
    "\n",
    "x_imp = imputers.get(\"EM\").fit_transform(x)\n",
    "\n",
    "imputers.get(\"softimpute\").fit_transform(x_miss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-dutch",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement towards Machine learning and AI for medicine, you can do so in the following ways!\n",
    "\n",
    "### Star HyperImpute on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the Repos! This helps raise awareness of the tools we're building.\n",
    "\n",
    "- [Star HyperImpute](https://github.com/vanderschaarlab/hyperimpute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f0841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperimpute",
   "language": "python",
   "name": "hyperimpute"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
