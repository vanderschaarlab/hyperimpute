{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec0aaa4",
   "metadata": {},
   "source": [
    "## Load reference datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d1233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/code/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#!pip install xlrd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy import signal\n",
    "\n",
    "def smooth_line(src: list) -> list:\n",
    "    return signal.savgol_filter(src, 3, 1)\n",
    "\n",
    "X_raw_diab, _ = load_diabetes(as_frame=True, return_X_y=True)\n",
    "\n",
    "X_raw_breast_cancer, _ = load_breast_cancer(as_frame=True, return_X_y=True)\n",
    "X_raw_california, _ = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X_raw_iris, y_raw_iris = load_iris(as_frame = True, return_X_y = True)\n",
    "\n",
    "climate_model_samples = np.loadtxt(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00252/pop_failures.dat\",\n",
    "    skiprows=1,\n",
    ")\n",
    "climate_model_df = pd.DataFrame(climate_model_samples)\n",
    "\n",
    "raw_datasets = {\n",
    "    \"airfoil\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/00291/airfoil_self_noise.dat\",\n",
    "        header=None,\n",
    "        sep=\"\\\\t\",\n",
    "    ),\n",
    "    \"blood\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\"\n",
    "    ),\n",
    "    \"bc\": X_raw_breast_cancer,\n",
    "    \"california\": X_raw_california,\n",
    "    \"climate\": climate_model_df,\n",
    "    \"compression\": pd.read_excel(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\"\n",
    "    ),\n",
    "    \"slump\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data\"\n",
    "    ),\n",
    "    \"sonar\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\",\n",
    "        header=None,\n",
    "    ),\n",
    "    \"diabetes\": X_raw_diab,\n",
    "    \"wine_red\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",\n",
    "        sep=\";\",\n",
    "    ),\n",
    "    \"wine_white\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\",\n",
    "        sep=\";\",\n",
    "    ),\n",
    "    \"yeast\": pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data\",\n",
    "        sep=\"\\s+\",\n",
    "        header=None,\n",
    "    ),\n",
    "    \"iris\": X_raw_iris,\n",
    "    \"libras\":pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/libras/movement_libras.data\",sep=\",\", header = None),\n",
    "    \"parkinsons\": pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\",sep=\",\"),\n",
    "    \"yacht\": pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\",sep=\"\\s+\", header = None),\n",
    "    \"ionosphere\": pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\",sep=\",\", header = None),\n",
    "    \"letter\": pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\", header = None),\n",
    "    \"spam\":pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"),\n",
    "    \"credit\":pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\", header = None),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ea27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6236a07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /code/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from hyperimpute.plugins.imputers import Imputers\n",
    "from hyperimpute.utils.distributions import enable_reproducible_results\n",
    "import hyperimpute.logger as log\n",
    "\n",
    "from benchmark_imputation import evaluate_dataset_repeated_internal\n",
    "\n",
    "enable_reproducible_results()\n",
    "\n",
    "imputers = Imputers()\n",
    "\n",
    "def get_imputer():\n",
    "    return imputers.get(\"hyperimpute\", \n",
    "        optimizer = \"simple\",\n",
    "        classifier_seed = [\"random_forest\", \"logistic_regression\", \"xgboost\", \"catboost\"],\n",
    "        regression_seed = [\"random_forest_regressor\", \"linear_regression\", \"xgboost_regressor\", \"catboost_regressor\",],\n",
    "    )\n",
    "\n",
    "        \n",
    "def evaluate_dataset_repeated(\n",
    "    name,\n",
    "    X_raw,\n",
    "    y,\n",
    "    ref_methods=[\"sklearn_ice\", \"sklearn_missforest\", \"sinkhorn\", \"miwae\", \"gain\"],\n",
    "    scenarios=[\"MAR\", \"MCAR\", \"MNAR\"],\n",
    "    miss_pct=[0.3],\n",
    "    n_iter=4,\n",
    "    debug=False,\n",
    "):\n",
    "    return evaluate_dataset_repeated_internal(\n",
    "        name = name,\n",
    "        evaluated_model = get_imputer(),\n",
    "        X_raw = X_raw,\n",
    "        y = y,\n",
    "        ref_methods=ref_methods,\n",
    "        scenarios=scenarios,\n",
    "        miss_pct=miss_pct,\n",
    "        n_iter=n_iter,\n",
    "        debug=debug,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211baab9",
   "metadata": {},
   "source": [
    "## By dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ca0189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15\n",
       "0      19   2   8   3   5   1   8  13   0   6   6  10   8   0   8   0\n",
       "1       8   5  12   3   7   2  10   5   5   4  13   3   9   2   8   4\n",
       "2       3   4  11   6   8   6  10   6   2   6  10   3   7   3   7   3\n",
       "3      13   7  11   6   6   3   5   9   4   6   4   4  10   6  10   2\n",
       "4       6   2   1   3   1   1   8   6   6   6   6   5   9   1   7   5\n",
       "...    ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
       "19995   3   2   2   3   3   2   7   7   7   6   6   6   4   2   8   3\n",
       "19996   2   7  10   8   8   4   4   8   6   9  12   9  13   2   9   3\n",
       "19997  19   6   9   6   7   5   6  11   3   7  11   9   5   2  12   2\n",
       "19998  18   2   3   4   2   1   8   7   2   6  10   6   8   1   9   5\n",
       "19999   0   4   9   6   6   2   9   5   3   1   8   1   8   2   7   2\n",
       "\n",
       "[20000 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\", header = None)\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == \"object\":\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "  \n",
    "last_col = df.columns[-1]\n",
    "y = df[last_col]\n",
    "X_raw = df.drop(columns = [last_col])\n",
    "\n",
    "X_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054355bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample already cached 1000\n",
      "subsample already cached 4000\n",
      "subsample eval 7000\n",
      "benchmark  hyperimpute 10.579314231872559\n",
      "benchmark  sklearn_ice 1.462017297744751\n",
      "benchmark  sklearn_missforest 42.84639096260071\n",
      "benchmark  sinkhorn 80.96199417114258\n",
      "benchmark  miwae 58.31982398033142\n",
      "benchmark  gain 4.742030382156372\n",
      "benchmark  hyperimpute 21.214603185653687\n",
      "benchmark  sklearn_ice 5.034836530685425\n",
      "benchmark  sklearn_missforest 45.510661125183105\n",
      "benchmark  sinkhorn 79.2300226688385\n",
      "benchmark  miwae 58.644336223602295\n",
      "benchmark  gain 4.746025323867798\n",
      "benchmark  hyperimpute 15.660754442214966\n",
      "benchmark  sklearn_ice 5.518197774887085\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results_by_df_size = {}\n",
    "for subsample in [1000, 4000, 7000, 10000, 13000, 17000, 20000]:\n",
    "    try:\n",
    "        with open(\"general_results/error_by_df_size.json\") as f:\n",
    "            results_by_df_size = json.load(f)\n",
    "    except BaseException:\n",
    "        pass\n",
    "    \n",
    "    if str(subsample) in results_by_df_size:\n",
    "        print(\"subsample already cached\", subsample)\n",
    "        continue\n",
    "        \n",
    "    print(\"subsample eval\", subsample)\n",
    "    X_local = X_raw.sample(subsample)\n",
    "    y_local = y[X_local.index]\n",
    "    \n",
    "    results_by_df_size[subsample] = evaluate_dataset_repeated(f\"subsample_{subsample}\", X_local, y_local)\n",
    "    \n",
    "    \n",
    "    with open(\"general_results/error_by_df_size.json\", \"w\") as f:\n",
    "        json.dump(results_by_df_size, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e173b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdc420b9",
   "metadata": {},
   "source": [
    "## By feature count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a62ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results_by_feat_count = {}\n",
    "\n",
    "for feat_count in [2,4,6,8,10,12,14]:\n",
    "    try:\n",
    "        with open(\"general_results/error_by_df_feat_count.json\") as f:\n",
    "            results_by_feat_count = json.load(f)\n",
    "    except BaseException:\n",
    "        pass\n",
    "    \n",
    "    if str(feat_count) in results_by_feat_count:\n",
    "        print(\"feat count cached\", feat_count)\n",
    "        continue\n",
    "    \n",
    "    print(\"eval feat count\", feat_count)\n",
    "        \n",
    "    X_local = X_raw.sample(2000)\n",
    "    y_local = y[X_local.index]\n",
    "    X_local = X_local.iloc[:, :feat_count]\n",
    "        \n",
    "    results_by_feat_count[feat_count] = evaluate_dataset_repeated(f\"feature_subsample_{feat_count}\", X_local, y_local)\n",
    "    \n",
    "    \n",
    "    with open(\"general_results/error_by_df_feat_count.json\", \"w\") as f:\n",
    "        json.dump(results_by_feat_count, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25575358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82838e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc02509f",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d587917",
   "metadata": {},
   "source": [
    "## Plot error by df size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1aa407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis = list(results_by_df_size.keys())\n",
    "x_axis = np.asarray(x_axis)\n",
    "\n",
    "fontsize = 14\n",
    "\n",
    "output_dir = Path(\"diagrams\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "map_keys = {\n",
    "    \"rmse\" : \"Mean RMSE\",\n",
    "     \"wasserstein\": \"Mean Wasserstein distance\"\n",
    "}\n",
    "def generate_plot_for_ax(ax, error_by_df_size_plots, metric):\n",
    "    for model in error_by_df_size_plots[metric]:\n",
    "        datapoints = error_by_df_size_plots[metric][model][\"mean\"]\n",
    "        datapoints_std = error_by_df_size_plots[metric][model][\"std\"]\n",
    "  \n",
    "        ax.errorbar(\n",
    "            np.asarray(x_axis),\n",
    "            smooth_line(np.asarray(datapoints)),\n",
    "            yerr = np.asarray(datapoints_std),\n",
    "            label=str(model),\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "\n",
    "    ax.set_xticks(x_axis, fontsize=fontsize)\n",
    "    ax.set_ylabel(map_keys[metric], fontsize=fontsize)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=fontsize)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def generate_plot(error_by_df_size_plots, scenario):\n",
    "    plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "    metrics = list([\"rmse\", \"wasserstein\"])\n",
    "    fig, axs = plt.subplots(len(metrics), figsize=(10, 11))\n",
    "\n",
    "    for idx, metric in enumerate(error_by_df_size_plots.keys()):\n",
    "        generate_plot_for_ax(axs[idx], error_by_df_size_plots, metric)\n",
    "\n",
    "    # plt.title(f\"{scenario} simulation with {miss} missingness\", fontdict = {\"fontsize\": 150}, loc = \"top\")\n",
    "\n",
    "    axs[0].legend(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.15, 1.22),\n",
    "        ncol=int(len(error_by_df_size_plots[\"rmse\"]) / 2),\n",
    "        prop={\"size\": fontsize},\n",
    "    )\n",
    "    fig.suptitle(f\"{scenario} simulation: error by dataset size\", fontsize=fontsize)\n",
    "    plt.savefig(output_dir / f\"error_by_df_size_{scenario}.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "error_by_df_size_plots = {\n",
    "    \"rmse\": {},\n",
    "    \"wasserstein\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for size in results_by_df_size:\n",
    "    headers = results_by_df_size[size][\"headers\"][2:]\n",
    "    headers[0] = \"hyperimpute\"\n",
    "    headers[1] = \"ice\"\n",
    "    headers[2] = \"missforest\"\n",
    "    \n",
    "    rmse = results_by_df_size[size][\"rmse\"][0][2:]\n",
    "    wasserstein = results_by_df_size[size][\"wasserstein\"][0][2:]\n",
    "    \n",
    "    for idx, header in enumerate(headers):\n",
    "        if header not in error_by_df_size_plots[\"rmse\"]:\n",
    "            error_by_df_size_plots[\"rmse\"][header] = {\"mean\" : [], \"std\" : []}\n",
    "            error_by_df_size_plots[\"wasserstein\"][header] = {\"mean\" : [], \"std\" : []}\n",
    "\n",
    "        error_by_df_size_plots[\"rmse\"][header][\"mean\"].append(rmse[idx][0])\n",
    "        error_by_df_size_plots[\"rmse\"][header][\"std\"].append(rmse[idx][1])\n",
    "        error_by_df_size_plots[\"wasserstein\"][header][\"mean\"].append(wasserstein[idx][0])\n",
    "        error_by_df_size_plots[\"wasserstein\"][header][\"std\"].append(wasserstein[idx][1])\n",
    "\n",
    "for scenario in [\"MAR\", \"MCAR\", \"MNAR\"]:\n",
    "    print(\"scenario\", scenario)\n",
    "    generate_plot(error_by_df_size_plots, scenario = scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced9dfb",
   "metadata": {},
   "source": [
    "# By feature size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_axis = list(results_by_feat_count.keys())\n",
    "x_axis = np.asarray(x_axis)\n",
    "\n",
    "fontsize = 14\n",
    "\n",
    "\n",
    "def generate_plot_for_ax(ax, error_by_feat_count_plots, metric):\n",
    "    ## We selected the same dataset from every model, and average across missingness pct\n",
    "    for model in error_by_feat_count_plots[metric]:\n",
    "        datapoints = error_by_feat_count_plots[metric][model][\"mean\"]\n",
    "        datapoints_std = error_by_feat_count_plots[metric][model][\"std\"]\n",
    "\n",
    "        ax.errorbar(\n",
    "            np.asarray(x_axis),\n",
    "            smooth_line(np.asarray(datapoints)),\n",
    "            yerr = np.asarray(datapoints_std),\n",
    "            label=str(model),\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(x_axis, fontsize=fontsize)\n",
    "    ax.set_ylabel(metric, fontsize=fontsize)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=fontsize)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def generate_plot(error_by_feat_count_plots, scenario):\n",
    "    plt.style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "\n",
    "    metrics = [\"rmse\", \"wasserstein\"]\n",
    "    fig, axs = plt.subplots(len(metrics), figsize=(10, 11))\n",
    "\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        generate_plot_for_ax(axs[idx], error_by_feat_count_plots, metric)\n",
    "\n",
    "    # plt.title(f\"{scenario} simulation with {miss} missingness\", fontdict = {\"fontsize\": 150}, loc = \"top\")\n",
    "\n",
    "    axs[0].legend(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.15, 1.22),\n",
    "        ncol=int(len(error_by_feat_count_plots[\"rmse\"]) / 2),\n",
    "        prop={\"size\": fontsize},\n",
    "    )\n",
    "    fig.suptitle(f\"{scenario} simulation: error by feature count\", fontsize=fontsize)\n",
    "    plt.savefig(output_dir / f\"error_by_feature_cnt_{scenario}.png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "error_by_feat_count_plots = {\n",
    "    \"rmse\": {},\n",
    "    \"wasserstein\": {}\n",
    "}\n",
    "\n",
    "\n",
    "for feat_count in results_by_feat_count:\n",
    "    headers = results_by_feat_count[feat_count][\"headers\"][2:]\n",
    "    headers[0] = \"hyperimpute\"\n",
    "    headers[1] = \"ice\"\n",
    "    headers[2] = \"missforest\"\n",
    "    \n",
    "    rmse = results_by_feat_count[feat_count][\"rmse\"][0][2:]\n",
    "    wasserstein = results_by_feat_count[feat_count][\"wasserstein\"][0][2:]\n",
    "    \n",
    "    for idx, header in enumerate(headers):\n",
    "        if header not in error_by_feat_count_plots[\"rmse\"]:\n",
    "            error_by_feat_count_plots[\"rmse\"][header] = {\"mean\" : [], \"std\" : []}\n",
    "            error_by_feat_count_plots[\"wasserstein\"][header] = {\"mean\" : [], \"std\" : []}\n",
    "\n",
    "        error_by_feat_count_plots[\"rmse\"][header][\"mean\"].append(rmse[idx][0])\n",
    "        error_by_feat_count_plots[\"rmse\"][header][\"std\"].append(rmse[idx][1])\n",
    "        error_by_feat_count_plots[\"wasserstein\"][header][\"mean\"].append(wasserstein[idx][0])\n",
    "        error_by_feat_count_plots[\"wasserstein\"][header][\"std\"].append(wasserstein[idx][1])\n",
    "        \n",
    "for scenario in [\"MAR\", \"MCAR\", \"MNAR\"]:\n",
    "    print(\"scenario \", scenario)\n",
    "    generate_plot(error_by_feat_count_plots, scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216739e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42fe3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
